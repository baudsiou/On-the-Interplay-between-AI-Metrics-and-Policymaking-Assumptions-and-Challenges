{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f981188-7614-4e0c-b589-39d82816a5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import spacy\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "#spacy.cli.download(\"en_core_web_sm\")\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models  \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c7e0738-fd68-462e-97fe-1d1ca60cee0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "my_dict = {} # Initializing dictionary to store document texts\n",
    "total_scores = {} # Initializing dictionary to store total scores\n",
    "country_topics = {} # Initializing dictionary to store topics for each country\n",
    "\n",
    "#Define function to read files from a directory\n",
    "def readFiles(file_path):\n",
    "    allFiles = listAllFiles(file_path)\n",
    "    for f in allFiles:\n",
    "        if '.DS' in f:\n",
    "            continue\n",
    "\n",
    "        pdf_file = open(os.path.join(file_path, f), 'r', encoding='utf-8')\n",
    "        to_pdf = pdf_file.read()\n",
    "        my_dict[f.replace('.txt', '')] = to_pdf\n",
    "        total_scores[f.replace('.txt', '')] = {}\n",
    "        country_topics[f.replace('.txt', '')] = {}\n",
    "\n",
    "directory_path = r'enter path here'\n",
    "\n",
    "# Defining function to list all files in a directory\n",
    "def listAllFiles(path):\n",
    "    return os.listdir(path)\n",
    "\n",
    "\n",
    "my_dict={}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "file_path=directory_path\n",
    "readFiles(file_path)\n",
    "\n",
    "# Initializing dictionary to store paragraph texts\n",
    "my_dict_paragraph={} \n",
    "\n",
    "#split into paragraphs based on two new line characters\n",
    "for k,i in my_dict.items():\n",
    "    sentences=i.split('\\n\\n')\n",
    "    for count in range(0,len(sentences)-1):\n",
    "        my_dict_paragraph[k + str(count)]=sentences[count]\n",
    "\n",
    "df['country'] = my_dict_paragraph.keys()\n",
    "df['text'] = my_dict_paragraph.values()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e8778d-5987-4df0-ab3b-229d4b2594e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for calculating the coherence score\n",
    "def coherence_score(dictionary, corpus, texts, start, limit, step):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "\n",
    "    for num_topics in range(start, limit, step):\n",
    "\n",
    "        print('Number of Topics: ', num_topics)\n",
    "        model = models.LdaModel(corpus, num_topics=num_topics,\n",
    "                                  id2word=dictionary_LDA,\n",
    "                                  passes=75, alpha=1,\n",
    "                                  eta=[0.01]*len(dictionary_LDA.keys()))\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "\n",
    "        # print the extracted topics\n",
    "        for i,topic in model.show_topics(formatted=True, num_topics=num_topics, num_words=20):\n",
    "            print(str(i)+\": \"+ topic)\n",
    "            print()\n",
    "        print(coherence_model.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "#define function for constructing bigrams\n",
    "def bigrams_construct(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "#define function for lemmatization\n",
    "def lemmatization(texts, allowed_postags=['NOUN','ADJ','ADV']):\n",
    "    \n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2fbe80-9a6e-4546-9b91-91e27e9c5986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopword removal \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stop_words = stopwords.words('english')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# country names and subsequent adjectives related to origin removal\n",
    "countries_stopwords = ['USA','america', 'US', 'arab','emirates','australia','austria','brazil','bulgaria', 'canada','china','czech','republic','denmark','egypt','estonia','finland','france','germany','hungary',\n",
    "'india','ireland','italy','japan','korea','lithuania','luxembourg','malta','mauritius','mexico','netherlands','norway','peru','poland','portugal','russia','saudi','arabia','serbia',\n",
    "'singapore','slovenia','spain','sweden','turkey','united','kingdom','uruguay','vietnam']\n",
    "countries_adj = ['english','german','spanish', 'austrian','swedish', 'brazilian','bulgarian','chinese','french','italian','norwegian','australian','russian','finnish','estonian','indian','estonian',\n",
    "                       'lithuanian','portuguese','maltese','american','slovenian','egyptian','mexican','danish','dutch','turkish','american', 'vietnamese','koreans', 'korean']\n",
    "stop_words.extend(countries_stopwords)\n",
    "stop_words.extend(countries_adj)\n",
    "stop_words.extend(['http','https','federal','artificial','intelligence', 'ai', '(ii)', '(i)', '(iii)', '(v)', '(iv)', '(vi)', '(vii)','(viii)'])\n",
    "stop_words.extend(spacy_stopwords)\n",
    "\n",
    "# Remove punctuation from the corpus\n",
    "df['clean_doc'] = df['text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Removing everything except alphabets\n",
    "df['clean_doc'] = df['clean_doc'].str.replace(r'[^a-zA-Z ]', '')\n",
    "\n",
    "# Remove short words - This is a list comprehension that filters out words with a length less than or equal to 3 characters.\n",
    "df['clean_doc'] = df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "# Convert all text to lowercase\n",
    "df['clean_doc'] = df['clean_doc'].apply(lambda x: x.lower())\n",
    "\n",
    "# Remove any numbers from each element in the series\n",
    "df['clean_doc'] = df['clean_doc'].replace(to_replace=r'\\d', value='', regex=True)\n",
    "\n",
    "# Text tokenization\n",
    "tokenized_doc = df['clean_doc'].apply(lambda x: x.split())\n",
    "df['tokenized_doc'] = df['clean_doc'].apply(lambda x: x.split())\n",
    "\n",
    "#Remove stop-words\n",
    "df['tokenized_doc'] = df['tokenized_doc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "# Remove rows with empty lists (paragraphs)\n",
    "df = df[df['tokenized_doc'].apply(lambda x: len(x) > 0)]\n",
    "# Reset the index and drop the existing index column\n",
    "df = df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f49f893f-7a3f-44e9-8e15-17136bf69590",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words = df['tokenized_doc']\n",
    "ps = PorterStemmer()\n",
    "#de-tokenization\n",
    "detokenized_doc = []\n",
    "for i in range(len(df)):\n",
    "    t = ' '.join(df['tokenized_doc'][i])\n",
    "    detokenized_doc.append(ps.stem(t))\n",
    "\n",
    "df['clean_doc2'] = detokenized_doc\n",
    "\n",
    "\n",
    "# Bigram/trigram detection\n",
    "bigram = gensim.models.Phrases(data_words,threshold=80) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=1)\n",
    "bigram_det = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_det = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "data_words_bigrams = bigrams_construct(data_words)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# keep only nouns, adjectives and adverbs\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN','ADJ','ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e284a1e5-7da3-47ec-83ef-691e581dfc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the LDA model\n",
    "dictionary_LDA = corpora.Dictionary(data_lemmatized)\n",
    "dictionary_LDA.filter_extremes()\n",
    "\n",
    "texts = data_lemmatized\n",
    "corpus = [dictionary_LDA.doc2bow(text) for text in texts]\n",
    "\n",
    "# Run the topic model for 20 topics and calculate the coherence score\n",
    "limit=21; start=2; step=1;\n",
    "model_list, coherence_values = coherence_score(dictionary=dictionary_LDA, corpus=corpus, texts=data_lemmatized, start=start, limit=limit, step=step)\n",
    "\n",
    "#  Topics graph with relevant coherence scores\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.savefig('coherenceScores.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
